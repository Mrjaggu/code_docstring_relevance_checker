{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "493cee78-9e12-4d34-8a2f-e33ebee2a995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "05e48d64-4f85-4e22-bc1b-7e247ad95673",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise data path\n",
    "data_path = \"/home/jovyan/input/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7c600550-716f-43be-8b43-5916c621ad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read train data\n",
    "data = pd.read_parquet(data_path + \"train.parquet\")[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "1af3905d-bed8-47d7-adc0-478f361fbe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data checking\n",
    "test_data = pd.read_parquet(data_path + \"test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "6fe7c731-01b5-4191-8505-58edced7ad1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def consume_messages(self, max_next_messages):\n",
      "        if self.__next_messages == 0:\n",
      "            self.set_next_messages(min(1000, max_next_messages))\n",
      "        self.set_next_messages(min(self.__next_messages, max_next_messages))\n",
      "        mark = time.time()\n",
      "        for record in self._get_messages_from_consumer():\n",
      "            yield record.partition, record.offset, record.key, record.value\n",
      "        newmark = time.time()\n",
      "        if newmark - mark > 30:\n",
      "            self.set_next_messages(self.__next_messages / 2 or 1)\n",
      "        elif newmark - mark < 5:\n",
      "            self.set_next_messages(min(self.__next_messages + 100, max_next_messages)) \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_data.code.iloc[4],\"\\n\"*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "3d439b88-a029-42ac-8919-5a60774f6716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get messages batch from Kafka (list at output)'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.docstring.iloc[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "088c9ed1-9db5-4194-8785-7d422a755b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3a7afb9e-7276-46af-84a7-ce2a87e2ca71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data.groupby('y_true')['id'].count().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "04b8a6f8-1bb5-4f96-84b1-ea5118bdc7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data['code_n_words']=sample_data['code'].apply(lambda row:len(row.split(\" \")))\n",
    "sample_data['doc_n_words']=sample_data['docstring'].apply(lambda row:len(row.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "5ee2748c-bebd-46bf-8b09-085ef0a286cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def normalized_word_Common(row):\n",
    "#         w1 = set(map(lambda word:word.lower().strip() , row['code'].split()))\n",
    "#         w2 = set(map(lambda word:word.lower().strip() , row['docstring'].split()))\n",
    "#         return 1.0*len(w1 & w2)\n",
    "    \n",
    "\n",
    "# def normalized_word_Total(row):\n",
    "#     w1 = set(map(lambda word: word.lower().strip(), row['code'].split()))\n",
    "#     w2 = set(map(lambda word: word.lower().strip(), row['docstring'].split()))    \n",
    "#     return 1.0 * (len(w1) + len(w2))\n",
    "\n",
    "\n",
    "# def normalized_word_share(row):\n",
    "#     w1 = set(map(lambda word: word.lower().strip(), row['code'].split()))\n",
    "#     w2 = set(map(lambda word: word.lower().strip(), row['docstring'].split()))    \n",
    "#     return 1.0 * len(w1 & w2)/(len(w1) + len(w2))\n",
    "\n",
    "# sample_data['word_common'] = sample_data.apply(\n",
    "#     normalized_word_Common , axis=1\n",
    "# )\n",
    "# sample_data['word_Total'] = sample_data.apply(\n",
    "#     normalized_word_Total, axis=1\n",
    "# )\n",
    "# sample_data['word_share'] = sample_data.apply(\n",
    "#     normalized_word_share, axis=1\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "6d03c064-2895-4aec-894b-dfce5b200df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dc6fcd68-c321-4eee-ade4-d51387e551bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of code with minimum length code : 0\n",
      "Number of docstring with minimum length doc : 1983\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of code with minimum length code :\", sample_data[sample_data['code_n_words']== 1].shape[0])\n",
    "print (\"Number of docstring with minimum length doc :\", sample_data[sample_data['doc_n_words']== 1].shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce24896-1eed-4ad8-8b46-8aff3664de0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "01ab42ee-ad49-4f5a-b728-59d8270fdefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data[(sample_data['doc_n_words']== 1) & (sample_data['y_true']== 0)]['code'].iloc[5].split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0dd41c3f-f32b-453e-a9f0-bd7e404bc177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "      <th>y_true</th>\n",
       "      <th>code_n_words</th>\n",
       "      <th>doc_n_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>d693dd78-c5a1-4465-be56-796bbf1c4a74</td>\n",
       "      <td>def extra_context(self, request, context):\\n  ...</td>\n",
       "      <td>判断两条线段是否交叉</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>4c5cd695-1b95-481d-8436-7c30ff0df298</td>\n",
       "      <td>def to_keypoint_image(self, size=1):\\n        ...</td>\n",
       "      <td>给出交易具体时间</td>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>c2a6ce73-b58a-42ba-91ff-a114000124cf</td>\n",
       "      <td>def opensearch(request):\\n    contact_email = ...</td>\n",
       "      <td>stub</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5351</th>\n",
       "      <td>0df5f855-eaa9-4b55-b1f8-cc110a6df8dd</td>\n",
       "      <td>def list_space_systems(self, page_size=None):\\...</td>\n",
       "      <td>https://picamera.readthedocs.io/en/release-1.1...</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5962</th>\n",
       "      <td>8f375898-d95b-4b36-a340-5d4c13d8c665</td>\n",
       "      <td>def get_identifiers_splitted_by_weights(identi...</td>\n",
       "      <td>Lower.</td>\n",
       "      <td>0</td>\n",
       "      <td>466</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        id  \\\n",
       "1154  d693dd78-c5a1-4465-be56-796bbf1c4a74   \n",
       "1657  4c5cd695-1b95-481d-8436-7c30ff0df298   \n",
       "1966  c2a6ce73-b58a-42ba-91ff-a114000124cf   \n",
       "5351  0df5f855-eaa9-4b55-b1f8-cc110a6df8dd   \n",
       "5962  8f375898-d95b-4b36-a340-5d4c13d8c665   \n",
       "\n",
       "                                                   code  \\\n",
       "1154  def extra_context(self, request, context):\\n  ...   \n",
       "1657  def to_keypoint_image(self, size=1):\\n        ...   \n",
       "1966  def opensearch(request):\\n    contact_email = ...   \n",
       "5351  def list_space_systems(self, page_size=None):\\...   \n",
       "5962  def get_identifiers_splitted_by_weights(identi...   \n",
       "\n",
       "                                              docstring  y_true  code_n_words  \\\n",
       "1154                                         判断两条线段是否交叉       0            25   \n",
       "1657                                           给出交易具体时间       0           267   \n",
       "1966                                               stub       0            85   \n",
       "5351  https://picamera.readthedocs.io/en/release-1.1...       0           128   \n",
       "5962                                             Lower.       0           466   \n",
       "\n",
       "      doc_n_words  \n",
       "1154            1  \n",
       "1657            1  \n",
       "1966            1  \n",
       "5351            1  \n",
       "5962            1  "
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data[(sample_data['doc_n_words']== 1) & (sample_data['y_true']== 0)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "49b67c57-e61b-4563-bd14-2269115d9207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_english_words(text):\n",
    "    new_string=re.sub('[^a-zA-Z0-9]',' ',text)\n",
    "    # print()\n",
    "    return new_string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3948a465-967e-4878-b224-7acf5ab1f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding function to clean https links\n",
    "import string\n",
    "\n",
    "def convert_html_text(text):\n",
    "    \"\"\" function convert http link to text\"\"\"\n",
    "    \n",
    "    if 'https://' in text:\n",
    "        for pattern in string.punctuation:\n",
    "            text = text.replace(pattern,\" \")\n",
    "\n",
    "        return text\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "b0141a4f-29de-4a17-9d2b-2dae14d84364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conveting hyperlinks to string format\n",
    "sample_data['docstring'] = sample_data['docstring'].apply(\n",
    "    convert_html_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "6a377da9-1231-4117-b86e-6e583556790e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "a7910e2b-8e44-4ad0-8f75-03a305178188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(lines):\n",
    "    cleaned = []\n",
    "    for line in lines.split(\"\\n\"):\n",
    "        line = re.sub(r\"([0-9]+)\",\"\", line)\n",
    "        line = re.sub(r\"([0-9]+)\",\"\", line)\n",
    "    \n",
    "        clean = re.sub(r\"\"\"\n",
    "               [,.;@#?!&$()|<~='^\\\\_`:>\"%/{}*]+  # Accept one or more copies of punctuation\n",
    "               \\ *           # plus zero or more copies of a space,\n",
    "               \"\"\",\n",
    "               \" \",          # and replace it with a single space\n",
    "               line, flags=re.VERBOSE)\n",
    "        #Manually handle cases not accepted by sub\n",
    "        clean = clean.replace(\"[\", \"\")\n",
    "        clean = clean.replace(\"+\", \"\")\n",
    "        clean = clean.replace(\"]\", \"\")\n",
    "        clean = clean.replace(\"-\", \"\")\n",
    "        # tokenize on white space\n",
    "        line = clean.split()\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "        # store as string\n",
    "        cleaned.append(' '.join(line))\n",
    "    # remove empty strings\n",
    "    return \" \".join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "98868b3d-18c0-4fc0-9c52-7318130678d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9726f29d-4119-4114-8918-486039326978",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stopwords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m SAFE_DIV \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0001\u001b[39m\n\u001b[0;32m----> 3\u001b[0m STOP_WORDS \u001b[38;5;241m=\u001b[39m \u001b[43mstopwords\u001b[49m\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_token_features\u001b[39m(q1, q2):\n\u001b[1;32m      6\u001b[0m     token_features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m10\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stopwords' is not defined"
     ]
    }
   ],
   "source": [
    "SAFE_DIV = 0.0001\n",
    "\n",
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "\n",
    "def get_token_features(q1, q2):\n",
    "    token_features = [0.0]*10\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "\n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "    # Get the non-stopwords in Questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords from Question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopwords from Question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common Tokens from Question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "\n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    \n",
    "    #Average Token Length of both Questions\n",
    "    token_features[9] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    return token_features\n",
    "\n",
    "# get the Longest Common sub string\n",
    "\n",
    "def get_longest_substr_ratio(a, b):\n",
    "    strs = list(distance.lcsubstrings(a, b))\n",
    "    if len(strs) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return len(strs[0]) / (min(len(a), len(b)) + 1)\n",
    "\n",
    "def extract_features(df):\n",
    "    # preprocessing each question\n",
    "    df[\"code\"] = df[\"code\"].fillna(\"\").apply(clean_data)\n",
    "    df[\"docstring\"] = df[\"docstring\"].fillna(\"\").apply(clean_data)\n",
    "\n",
    "    print(\"token features...\")\n",
    "    \n",
    "    # Merging Features with dataset\n",
    "    token_features = df.apply(lambda x: get_token_features(x[\"code\"], x[\"docstring\"]), axis=1)\n",
    "    \n",
    "    df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "    df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "    df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "    df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "    df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "    df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "    df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "    df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))\n",
    "    df[\"abs_len_diff\"]  = list(map(lambda x: x[8], token_features))\n",
    "    df[\"mean_len\"]      = list(map(lambda x: x[9], token_features))\n",
    "    \n",
    "    print(\"Feature creation is done\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d441bca4-b6cd-4d4b-9e0d-34a1edd1c933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 9.3 µs\n",
      "token features...\n",
      "Feature creation is done\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "sample_data = extract_features(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "a9de6995-02bd-4783-8ca4-f3ceb6ace44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "46b37457-e128-4f9a-b9d6-b8e82047cf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_words(text):\n",
    "    \"\"\" Removing words whose length less then 3 or equal to\"\"\"\n",
    "    \n",
    "    text = [x for x in text.split() if len(x)>3]\n",
    "    \n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "db370e32-0ac7-4de4-8e38-56d8b149b853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 9.54 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "sample_data['code'] = sample_data['code'].apply(remove_words)\n",
    "sample_data['docstring'] = sample_data['docstring'].apply(remove_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "0381a1eb-1808-43cd-859e-c64cce2c5007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(remove_non_english_words(\n",
    "# sample_data[(sample_data['doc_n_words']== 1) & (sample_data['y_true']== 1)]['docstring'].iloc[7]\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "bfaebeac-6b60-4b48-b454-6db35867fd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 0 ns, total: 4 µs\n",
      "Wall time: 9.3 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "sample_data['code_processed'] = sample_data['code'].apply(remove_non_english_words)\n",
    "sample_data['docstring_processed'] = sample_data['docstring'].apply(remove_non_english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "75bb1da9-909f-4a36-bf67-90c0b9593167",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    " \n",
    "# Create WordNetLemmatizer object\n",
    "wnl = WordNetLemmatizer()\n",
    "def getLemmatize_word(text):\n",
    "    \n",
    "    return \" \".join([wnl.lemmatize(words) for words in text.split(\" \")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f1494360-a4d0-462c-81fd-d446f6dfac37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>code</th>\n",
       "      <th>docstring</th>\n",
       "      <th>y_true</th>\n",
       "      <th>codelen</th>\n",
       "      <th>docstringlen</th>\n",
       "      <th>code_n_words</th>\n",
       "      <th>doc_n_words</th>\n",
       "      <th>word_common</th>\n",
       "      <th>word_Total</th>\n",
       "      <th>...</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>token_set_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66236bf5-ce26-4401-82e6-1660c718008f</td>\n",
       "      <td>def get uids self filename none self update re...</td>\n",
       "      <td>return a list of uids filename unused for api ...</td>\n",
       "      <td>1</td>\n",
       "      <td>143</td>\n",
       "      <td>77</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.199999</td>\n",
       "      <td>0.499975</td>\n",
       "      <td>0.249994</td>\n",
       "      <td>0.363633</td>\n",
       "      <td>0.199999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.5</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b6a4ef2c-8109-486c-a9ad-4f20d2c95fb2</td>\n",
       "      <td>def purge cdn object self container obj email ...</td>\n",
       "      <td>removes a cdnenabled object from public access...</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>337</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.159999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.199999</td>\n",
       "      <td>0.072727</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>37.5</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id  \\\n",
       "0  66236bf5-ce26-4401-82e6-1660c718008f   \n",
       "1  b6a4ef2c-8109-486c-a9ad-4f20d2c95fb2   \n",
       "\n",
       "                                                code  \\\n",
       "0  def get uids self filename none self update re...   \n",
       "1  def purge cdn object self container obj email ...   \n",
       "\n",
       "                                           docstring  y_true  codelen  \\\n",
       "0  return a list of uids filename unused for api ...       1      143   \n",
       "1  removes a cdnenabled object from public access...       1      145   \n",
       "\n",
       "   docstringlen  code_n_words  doc_n_words  word_common  word_Total  ...  \\\n",
       "0            77            10           12          2.0        22.0  ...   \n",
       "1           337             8           54          0.0        54.0  ...   \n",
       "\n",
       "    cwc_max   csc_min   csc_max   ctc_min   ctc_max  last_word_eq  \\\n",
       "0  0.199999  0.499975  0.249994  0.363633  0.199999           0.0   \n",
       "1  0.159999  0.000000  0.000000  0.199999  0.072727           0.0   \n",
       "\n",
       "   first_word_eq  abs_len_diff  mean_len  token_set_ratio  \n",
       "0            0.0           9.0      15.5               55  \n",
       "1            0.0          35.0      37.5               59  \n",
       "\n",
       "[2 rows x 22 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "321cfab7-926d-490f-9019-bfbb4f26b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "sample_data['code'] = sample_data['code'].apply(getLemmatize_word)\n",
    "sample_data['docstring'] = sample_data['docstring'].apply(getLemmatize_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "3d3ac64b-6a0c-49f1-becc-8118ed0dd195",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'uids self filename none self update return abook self bookentry entry self book sections'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data.code.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "5dd662d9-f318-48b4-8a6f-786df0fed3e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'meter', 'prison_term', 'clip', 'clock_time', 'time', 'clock', 'fourth_dimension', 'metre', 'sentence'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "def synonym_antonym_extractor(phrase):\n",
    "     \n",
    "    synonyms = []\n",
    "    antonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(phrase):\n",
    "        for l in syn.lemmas():\n",
    "            synonyms.append(l.name())\n",
    "            if l.antonyms():\n",
    "                antonyms.append(l.antonyms()[0].name())\n",
    "\n",
    "    print(set(synonyms))\n",
    "    print(set(antonyms))\n",
    "\n",
    "synonym_antonym_extractor('time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52c7e8e4-5493-4ebb-b0f2-f624cba22fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing modules\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "ps = PorterStemmer() #creating an instance of the class\n",
    "\n",
    "def get_stemming_word(text):\n",
    "    \n",
    "    return \" \".join([ps.stem(words) for words in text.split(\" \")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ef573597-79a1-492e-a9af-67abb72edbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using word_tokenize\n",
    "!pip install -q tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "aaf3837a-987c-49be-a9d4-892f70e54576",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data['code'] = sample_data['code'].apply(get_stemming_word)\n",
    "sample_data['docstring'] = sample_data['docstring'].apply(get_stemming_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "6f54e074-1740-4d21-88f8-e672d49386ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data.to_csv(\"processed_data2.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca6c1d9-8b02-42d1-9f14-e3424aaf259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee0036b-751e-4c4e-9ea3-112c692f9c3f",
   "metadata": {},
   "source": [
    "## Method name feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "9f045451-4f5c-467d-b4b7-0c0df78be64b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_uids'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.code.iloc[0].split(\":\")[0].split(\"def\")[-1].strip().split(\"(\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "cb3efd5d-4da8-4a39-a2b3-583acae34d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_method_name(code:str)->str:\n",
    "    \"\"\"Extracting method name from method name as a feature\"\"\"\n",
    "    \n",
    "    return code.split(\":\")[0].split(\"def\")[-1].strip().split(\"(\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390b48f-ea41-42f6-bc20-82153a314eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlops_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa63b59a764d65dc023eaa73261b648bfb1fb8ca9895367380803bca4d96270f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
