{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "865bec30-6cb1-4814-9e71-c789e0b74afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from sklearn.metrics import classification_report\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f095556a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef76361a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm\n",
    "\n",
    "lightgbm.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9c80746-df4f-48cf-b534-c4348255113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing libraries\n",
    "!pip install -q lightgbm # lgbm model\n",
    "!pip install -q sentence-transformers # sentence-transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e3cc15-a599-4264-8956-ad636e5c04fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae9b401-d7e9-4478-a3fb-c2b43f6671da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intializing for parallel computing of embedding \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "26043cc5-7a82-4e23-995f-93c9c937f0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using all-MiniLM-L6-v2 model which has overall 58 rogue-L score with 384 dim output size with\n",
    "# max length 128\n",
    "\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da4acdc2-2e5b-4119-bf81-9e7b07bcfe3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "\n",
    "# Do not change this cell\n",
    "test_path=\"/home/jovyan/input/data/test.parquet\"\n",
    "\n",
    "test_data = pd.read_parquet(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "205c2154-daa7-4cc2-9bcf-82886665533f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This below feature helped to increase score by 0.0020-> \n",
    "# from 0.9522 to 0.9538\n",
    "\n",
    "def get_method_name(code:str)->str:\n",
    "    \"\"\"Extracting method name from method name as a feature\"\"\"\n",
    "    \n",
    "    return code.split(\":\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7d611d24-50ef-465d-aad9-b8a567e4a6c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['method_feature'] = test_data['code'].apply(get_method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "210425b1-5132-4214-bc49-bb595cc40952",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS = stopwords.words(\"english\")\n",
    "\n",
    "# some custom stopwords depending on the analysis what we captured\n",
    "custom_stowords_list = ['def','self','returns','return',\n",
    "                        'zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz',\n",
    "                       'zzzzzzzzzz','zzzzzzzz','zzzzzzz','zzzzz',\n",
    "                       'aaaabnzacyceaaaadaqabaaabaqddurxdpyhoquyodutxjduzqmjyjqjszt',\n",
    "       'aaaabnzacyceaaaadaqabaaabaqdcgxehzf', 'aaaabnzacyceaaa',\n",
    "       'aaaabnzacyce', 'aaaabnzacyc', 'aaaabnzacy',\n",
    "       'aaaabnzackcmaaacbakdpkarimlm', 'aaaabbbccdaabbb', 'aaaabbbcca',\n",
    "       'aaaabaaacaaadaaaeaaafaaagaaahaaaiaaajaaakaaalaaamaaanaaaoaaapaaaqaaaraaasaaataaa',\n",
    "       'aaaaargh', 'aaaaabaaa', 'aaaaabaa',\n",
    "       'aaaaaaeceeeeiiiidnoooooouuuuysaaaaaaaceeeeiiii', 'aaaaaabaedaa',\n",
    "       'aaaaaaaarge', 'aaaaaaaalaaaaaaadwpwaaaaaaaaaa',\n",
    "       'aaaaaaaalaaaaaaadwpqzmzmzmdk', 'aaaaaaaadjuqwsqicqdwlclimq',\n",
    "       'aaaaaaaaaaagaagaaagaaa', 'aaaaaaaaaaaahaaaaaaaaa',\n",
    "       'aaaaaaaaaaaaaaaaaadwpwaaaaaaapc',\n",
    "       'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa',\n",
    "       'aaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaa',\n",
    "       'aaaaaaaaaaaaa', 'aaaaaaaaaaaa', 'aaaaaaaaaaa', 'aaaaaaaaaa',\n",
    "       'aaaaaaaaa', 'aaaaaaaa', 'aaaaaaa', 'aaaaaa', 'aaaaa', 'aaaa',\n",
    "       'aaa', 'aa']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eb2a9e68-a4db-4169-8010-92dd584284e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS.extend(custom_stowords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eecf1de0-8d90-4428-9157-72e0ce160147",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(x):\n",
    "    \"\"\"removing stopwords and numeric from string \"\"\"\n",
    "    x = str(x).lower()\n",
    "    y = []\n",
    "    for zz in x.split():\n",
    "        if zz.lower() not in STOP_WORDS or zz.lower() not in custom_stowords_list:\n",
    "            y.append(zz)\n",
    "    x = \" \".join(y)        \n",
    "    x = re.sub(r\"([0-9]+)\",\"\", x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def clean_data(lines):\n",
    "    \"\"\"\" Method return cleaned string\"\"\"\n",
    "    cleaned = []\n",
    "    for line in lines.split(\"\\n\"):\n",
    "        clean = re.sub(r\"\"\"\n",
    "               [,.;@#?!&$()|^<='\\\\_`:>\"%/{}*]+  # Accept one or more copies of punctuation\n",
    "               \\ *           # plus zero or more copies of a space,\n",
    "               \"\"\",\n",
    "               \" \",          # and replace it with a single space\n",
    "               line, flags=re.VERBOSE)\n",
    "        # clean = re.sub(r\"[^a-zA-Z0-9 ]+\",\"\",clean)\n",
    "        #Manually handle cases not accepted by sub\n",
    "        clean = clean.replace(\"[\", \"\")\n",
    "        clean = clean.replace(\"+\", \"\")\n",
    "        clean = clean.replace(\"]\", \"\")\n",
    "        clean = clean.replace(\"-\", \"\")\n",
    "        # tokenize on white space\n",
    "        line = clean.split()\n",
    "        # convert to lower case\n",
    "        line = [word.lower() for word in line]\n",
    "        # store as string\n",
    "        cleaned.append(' '.join(line))\n",
    "    # remove empty strings\n",
    "    return \" \".join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "158d722a-6cf6-4fb7-a4c1-98781fe6ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processing_pipeline(df):\n",
    "    \"\"\" pre-processing pipeline to clean the string \"\"\"\n",
    "\n",
    "    def convert_html_text(text):\n",
    "        \"\"\" function convert http link to text\"\"\"\n",
    "\n",
    "        if 'https://' in text:\n",
    "            for pattern in string.punctuation:\n",
    "                text = text.replace(pattern,\" \")\n",
    "\n",
    "            return text\n",
    "        else:\n",
    "            return text\n",
    "    \n",
    "    \n",
    "    def extract_features(df):\n",
    "        # preprocessing each question\n",
    "        print(f\"[Info] Running cleaning pipeline!\")\n",
    "        \n",
    "        df[\"code\"] = df[\"code\"].fillna(\"\").apply(clean_data)\n",
    "        df[\"docstring\"] = df[\"docstring\"].fillna(\"\").apply(clean_data)\n",
    "        df[\"method_feature\"] = df[\"method_feature\"].fillna(\"\").apply(clean_data)\n",
    "        \n",
    "        print(f\"[Info] Cleaning done!\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    ##conveting hyperlinks to string format\n",
    "    df['docstring'] = df['docstring'].apply(\n",
    "        convert_html_text\n",
    "    )\n",
    "    \n",
    "    df = extract_features(df)\n",
    "    \n",
    "    print(f\"[Info] Running stopwords removal pipeline!\")\n",
    "    \n",
    "    df['code'] = df['code'].apply(preprocess)\n",
    "    df['docstring'] = df['docstring'].apply(preprocess)\n",
    "    df[\"method_feature\"] = df[\"method_feature\"].apply(preprocess)\n",
    "    \n",
    "    print(f\"[Info] Stopwords removal complete!\")\n",
    "    \n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f2ea59ea-5e09-4a4c-911a-119c3cb3a328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 18 µs, sys: 0 ns, total: 18 µs\n",
      "Wall time: 59.6 µs\n",
      "[Info] Running cleaning pipeline!\n",
      "[Info] Cleaning done!\n",
      "[Info] Running stopwords removal pipeline!\n",
      "[Info] Stopwords removal complete!\n"
     ]
    }
   ],
   "source": [
    "# running pre-porcessing pipeline\n",
    "%time\n",
    "\n",
    "test_data = pre_processing_pipeline(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8a8887e9-aec2-44a9-b7f9-de07c9677b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "# This below similarity feature increased the score by 0.9\n",
    "# https://stackoverflow.com/questions/15173225/calculate-cosine-similarity-given-2-sentence-strings\n",
    "\n",
    "def counter_cosine_similarity(c1,c2):\n",
    "    \"\"\"method return cosine similarity between two strings\"\"\"\n",
    "    \n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k,0)*c2.get(k,0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k,0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k,0)**2 for k in terms))\n",
    "    \n",
    "    try:\n",
    "        return dotprod/(magA*magB)\n",
    "    except:\n",
    "        return 0\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3c10298d-bf49-4e77-9a34-3ce3170df00c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1 µs, total: 4 µs\n",
      "Wall time: 5.96 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "test_data['sim_score'] = test_data.apply(lambda x: \n",
    "                               counter_cosine_similarity(Counter(x['code'].split(\" \")),\n",
    "                                                        Counter(x['docstring'].split(\" \"))),\n",
    "                               axis=1\n",
    "                              )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19504e90-b7af-47cd-aa6d-22824b289fb5",
   "metadata": {},
   "source": [
    "## Calculating embedding of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4843133e-ecff-4172-a722-c37146c1f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embedding(list_of_sentences:pd.Series)->np:\n",
    "    \"\"\" method return array of sentence embedding \"\"\"\n",
    "    \n",
    "    feature_embedding = model.encode(list_of_sentences,\n",
    "                                     show_progress_bar=True\n",
    "                                    )\n",
    "    \n",
    "    return feature_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd216bc1-f99b-46e8-9db3-6f81a82cfeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_embedding_ = get_sentence_embedding(test_data.code.values)\n",
    "code_embedding_feature = get_sentence_embedding(test_data.method_feature.values)\n",
    "doc_embedding_ = get_sentence_embedding(test_data.docstring.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "343fd0da-2676-48de-8f59-939ed6c0d03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62df3a81de9e4cd1b8411092f55a3006",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # code features\n",
    "# code_embeds = test_data.code.values\n",
    "# code_embedding_ = model.encode(code_embeds,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6384b23d-5ed3-4708-980e-9856745d6ba2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e3667a491fc42ad927c51884681a2d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # code method feature\n",
    "# code_feature = test_data.method_feature.values\n",
    "# code_embedding_feature = model.encode(code_feature,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4fdd4d13-da54-4a63-b150-b63925455d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2213f630066143279d30de42677191a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# # docsting feature\n",
    "# doc_embeds = test_data.docstring.values\n",
    "# doc_embedding_ = model.encode(doc_embeds,show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd151327-e69a-47e4-87d0-30a4147c0588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "def data_prepare(code_embedding_feature:numpy,\n",
    "                doc_embedding_:numpy,\n",
    "                code_embedding_:numpy,\n",
    "                test_data:pd.DataFrame())->numpy:\n",
    "    \n",
    "    # concatenating our code, docsting, code method feature in one single numpy array\n",
    "    train_c = np.concatenate([code_embedding_feature,\n",
    "                              doc_embedding_,\n",
    "                              code_embedding_],\n",
    "                             axis=1)\n",
    "    \n",
    "    # using our features and converting it to numpy to add in embedding features\n",
    "    features_array = test_data[['sim_score']].to_numpy()\n",
    "    \n",
    "    # concatenating similarity feature to embedding feature\n",
    "    train_c_with_features = np.concatenate(\n",
    "        [train_c,features_array],\n",
    "        axis=1)\n",
    "    \n",
    "    return train_c_with_features\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "57fbf6f2-32e7-48e6-b417-f278698b18db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concatenating our code, docsting, code method feature in one single numpy array\n",
    "# train_c = np.concatenate([code_embedding_feature,\n",
    "#                           doc_embedding_,\n",
    "#                           code_embedding_],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "802cfcee-d9b7-4f08-a0ba-87f0dd70e716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using our features and converting it to numpy to add in embedding features\n",
    "# features_array = test_data[['sim_score']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3c341e3f-9ca9-47cd-b2ad-4da2802e16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concatenating similarity feature to embedding feature\n",
    "# train_c_with_features = np.concatenate(\n",
    "#     [train_c,features_array],\n",
    "#     axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cac57800-f525-4ab8-853c-3a2b1ee5f4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading our fine-tuned lgbm model\n",
    "lgb_model = joblib.load('lgbm_sentence_model_whole_data_with_sim_2000.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535cfcd-ca8d-4046-85e4-db7b06ac5336",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(train_c_with_features):\n",
    "    #use loaded model to to get predictions\n",
    "    pred_prob = lgb_model.predict(train_c_with_features)\n",
    "    # probability\n",
    "    pred_class = (pred_prob >=0.5)*1\n",
    "    test_data['y_pred'] = pred_class\n",
    "    submission_file = test_data[['id','y_pred']] # only the id and y_pred to be part of output\n",
    "    \n",
    "    return submission_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f41d1-67f7-41b7-8902-d6e8998ccdc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_file = get_prediction(train_c_with_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fdd7e019-801e-46ee-9215-d0057b5a435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #use loaded model to to get predictions\n",
    "# pred_prob = lgb_model.predict(train_c_with_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8acdc963-7dc2-4d92-9352-67821ffefded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # probability\n",
    "# pred_class = (pred_prob >=0.5)*1\n",
    "# test_data['y_pred'] = pred_class\n",
    "# submission_file = test_data[['id','y_pred']] # only the id and y_pred to be part of output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b268da4e-6181-4454-8352-b4d85495b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#final cell needs to create an output csv file with id and y_pred columns\n",
    "submission_file.to_csv('lgbm_sentence_model_whole_data_with_sim_2000_result.csv', \n",
    "                       index = False) # index = False is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a03bee-35bd-47a1-8ac4-a76ead6bc94c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('mlops_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa63b59a764d65dc023eaa73261b648bfb1fb8ca9895367380803bca4d96270f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
